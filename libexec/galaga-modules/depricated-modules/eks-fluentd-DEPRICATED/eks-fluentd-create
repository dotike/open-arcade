#!/usr/bin/env python3
# -*- mode: python -*-
# -*- coding: utf-8 -*-

'''
eks-fluentd-create -- GALAGA create fluentd componet
'''

# @depends: python (>=3.9)
__version__ = '0.1'
__author__ = 'Addepar Infrastructure Platform Tools Team <iptools@addepar.com>'
__description__ = "Create the ASTEROIDS infrastructure."

import argparse
import logging
import os
import sys
import yaml
import subprocess
import boto3
import docker
import base64
import time

from kubernetes.client.rest import ApiException
from kubernetes import client
from botocore.exceptions import ClientError
from pprint import pprint
from arclib import common, log, storage, k8s, common, ecr, eks


def create_repo(arcade_name: str, repository_name: str) -> bool:
    """Creates a ECR Repository in AWS

    Args:
        arcade_name (str): Name of the arcade
        repository_name (str): name of the repository

    Returns:
        bool: True if repo was created, False if it failed to create
    """
    current_region = common.get_arcade_region(arcade_name)
    ecr_client = boto3.client('ecr', region_name=current_region)
    try:
        ecr_client.create_repository(repositoryName=repository_name)
        return True
    except ecr_client.exceptions.RepositoryAlreadyExistsException:
        return False


def get_nodegroup_status(cluster_name: str, arcade_name: str) -> str:
    """Gets the Status of the Nodegroup for a EKS cluster

    Args:
        cluster_name (str): Name of the EKS cluster
        arcade_name (str): Name of the Arcade

    Returns:
        str: a string rep of the status
    """
    format_arcade_name = arcade_name.replace('.', '-')
    nodegroup_name = f"asteroids_nodegroup-{format_arcade_name}"
    eks_client = boto3.client('eks')
    try:
        response = eks_client.describe_nodegroup(clusterName=cluster_name, nodegroupName=nodegroup_name)
        return response['nodegroup']['status']
    except ClientError as e:
        return 'Not Active'
    

def get_eks_status(cluster_name: str):
    """
    Return the status of a EKS cluster.

    Args:
        cluster_name: cluster name

    Returns:
        status dict of the response or exception dict
    """
    eks_client = boto3.client('eks')
    try:
        response = eks_client.describe_cluster(name=cluster_name)
    except ClientError as c_e:
        return 'Not Active'

    return response['cluster']['status']


def build_fluentd_container() -> bool:
    """Builds the Fluentd Docker Image

    Returns:
        bool: True if container was built, False if not
    """
    client = docker.from_env()
    try:
        client.images.build(
            path='libexec/galaga-modules/eks-fluentd/fluentd-kubernetes-daemonset/debian-syslog',
            tag='arcade/fluentd-kubernetes-syslog')
        return True
    except client.errors.BuildError:
        return False
    except client.errors.APIError:
        return False
    
def check_fluentd_present():
    """Checks to make sure Fluentd Container is in ECR

    Returns:
        _type_: True if container is in ECR, False if not.
    """
    repo = 'arcade/fluentd-kubernetes-syslog'
    client = boto3.client('ecr')
    try:
        client.describe_repositories(repositoryNames=[repo])
        return True
    except client.exceptions.RepositoryNotFoundException:
        return False
    

def login_to_ecr_and_push_image(arcade_name: str):
    """Authenticate to ECR Registery, Build Fluentd Image for ECR, and push image to ECR.

    Args:
        arcade_name (str): Name of the Arcade

    Returns:
        bool: True if success, False if not.
    """
    repo_docker_image = 'arcade/fluentd-kubernetes-syslog'
    
    if not check_fluentd_present():
        pprint('fluentd repo in ecr is missing..... creating now')
        if not create_repo(arcade_name=arcade_name, repository_name=repo_docker_image):
            pprint('Error, Failed to create ecr repository for fluentd')
            return False
    
    docker_client = docker.from_env()
    current_region = common.get_arcade_region(arcade_name)
    ecr_client = boto3.client('ecr', region_name=current_region)
    token = ecr_client.get_authorization_token()
    username, password = base64.b64decode(token['authorizationData'][0]['authorizationToken']).decode().split(':')
    registry = token['authorizationData'][0]['proxyEndpoint'].replace("https://", "")
    docker_login = f"docker login -u {username} -p {password} {registry}"
    p = subprocess.Popen([docker_login], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, shell=True)
    p.communicate()
        
    login = docker_client.login(username, password, registry=registry)
    aws_account_num = boto3.client('sts').get_caller_identity().get('Account')
    ecr_tag = f'{aws_account_num}.dkr.ecr.{current_region}.amazonaws.com/{repo_docker_image}'
    
    if login['Status'] == 'Login Succeeded':
        lets_build_fluentd_locally = build_fluentd_container()
        if lets_build_fluentd_locally:
            find_fluentd_img = docker_client.images.get(name='arcade/fluentd-kubernetes-syslog:latest')
            tag = find_fluentd_img.tag(repository=ecr_tag, tag='latest')
            if tag:
                docker_client.images.push(f"{ecr_tag}", tag='latest')
                docker_client.images.remove(image=ecr_tag, force=True)
                docker_client.images.remove(image='arcade/fluentd-kubernetes-syslog:latest', force=True)
                return True
            else:
                return False
        else:
            return False


def apply_asteroids_fluentd_daemonset(arcade_name: str, cluster_prefix: str) -> bool:
    """
    Apply ServiceAccount, ClusterRole, ClusterRoleBinding, and DaemonSet for
    fluentd node logging to log-relay.

    Args:
        arcade_name: arcade_name name
        cluster_prefix: the prefix of a cluster

    Returns:
        bool
    """
    if not cluster_prefix or not arcade_name:
        return False

    arcade_domain = arcade_name.replace('_', '-')
    arcade_safe_name = arcade_name.replace('_', '').replace('.', '-')
    asg_name = f"{cluster_prefix}.{arcade_domain}"
    daemonset_name = f"node-collector-{arcade_safe_name}"
    
    fluentd_syslog_uri = ecr.get_container_uri('arcade/fluentd-kubernetes-syslog')
    logging.info(fluentd_syslog_uri)
    
    if not fluentd_syslog_uri:
        logging.error('No arcade/fluentd-kubernetes-syslog container in ECR')
        return False
    
    try:
        k8s.load_arcade_k8s_config(arcade_name)
    except ClientError:
        return False

    core_v1 = client.CoreV1Api()
    rbac_v1 = client.RbacAuthorizationV1Api()
    apps_v1 = client.AppsV1Api()

    serviceaccount_yaml = """
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fluentd
  namespace: kube-system
  labels:
    k8s-app: fluentd-logging
    version: v1
"""
    service_account = yaml.safe_load(serviceaccount_yaml)

    try:
        response = core_v1.create_namespaced_service_account(
            namespace=service_account['metadata']['namespace'],
            body=service_account,
        )
    except ApiException as api_error:
        if api_error.status == 409:
            api_response = core_v1.replace_namespaced_service_account(
                name=service_account['metadata']['name'],
                namespace=service_account['metadata']['namespace'],
                body=service_account,
            )
        elif api_error.status == 401:
            pass
        else:
            raise api_error

    clusterrole_yaml = """
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: fluentd
rules:
- apiGroups:
  - ''
  resources:
  - pods
  - namespaces
  verbs:
  - get
  - list
  - watch
"""
    cluster_role = yaml.safe_load(clusterrole_yaml)

    try:
        response = rbac_v1.create_cluster_role(
            body=cluster_role,
        )
    except ApiException as api_error:
        if api_error.status == 409:
            api_response = rbac_v1.replace_cluster_role(
                name=cluster_role['metadata']['name'],
                body=cluster_role,
            )
        elif api_error.status == 401:
            pass
        else:
            raise api_error

    clusterrolebinding_yaml = """
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: fluentd
roleRef:
  kind: ClusterRole
  name: fluentd
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: fluentd
  namespace: kube-system
"""
    cluster_role_binding = yaml.safe_load(clusterrolebinding_yaml)

    try:
        response = rbac_v1.create_cluster_role_binding(
            body=cluster_role_binding,
        )
    except ApiException as api_error:
        if api_error.status == 409:
            api_response = rbac_v1.replace_cluster_role_binding(
                name=cluster_role_binding['metadata']['name'],
                body=cluster_role_binding,
            )
        elif api_error.status == 401:
            pass
        else:
            raise api_error

    daemonset_yaml = f"""
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: {daemonset_name}
  namespace: kube-system
  labels:
    k8s-app: fluentd-logging
    version: v1
spec:
  selector:
    matchLabels:
      k8s-app: fluentd-logging
  template:
    metadata:
      labels:
        k8s-app: fluentd-logging
        version: v1
    spec:
      serviceAccount: fluentd
      serviceAccountName: fluentd
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      containers:
      - name: {daemonset_name}
        image: {fluentd_syslog_uri}
        env:
          - name:  SYSLOG_HOST
            value: '{asg_name}'
          - name:  SYSLOG_PORT
            value: '514'
          - name:  SYSLOG_PROTOCOL
            value: 'udp'
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: dockercontainerlogdirectory
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: dockerpodlogdirectory
          mountPath: /var/log/pods
          readOnly: true
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: dockercontainerlogdirectory
        hostPath:
          path: /var/lib/docker/containers
      - name: dockerpodlogdirectory
        hostPath:
          path: /var/log/pods
"""
    daemon_set = yaml.safe_load(daemonset_yaml)

    try:
        response = apps_v1.create_namespaced_daemon_set(
            namespace=daemon_set['metadata']['namespace'],
            body=daemon_set,
        )
    except ApiException as api_error:
        if api_error.status == 409:
            api_response = apps_v1.replace_namespaced_daemon_set(
                namespace=daemon_set['metadata']['namespace'],
                name=daemon_set['metadata']['name'],
                body=daemon_set,
            )
        elif api_error.status == 401:
            pass
        else:
            raise api_error

    return True



def main():
    """_summary_
    """
    parser = argparse.ArgumentParser(description=main.__doc__)
    parser.add_argument("-A", "--arcade", help='Arcade Name')
    parser.add_argument("--gsd", action="store_true", help="Use Account bucket not ARCADE bucket")
    parser.add_argument("-p", "--path", help="Full path to filename in s3", required=True)
    
    log.add_log_level_argument(parser)
    args = parser.parse_args()
    log.set_log_level(args.verbose)
    
    if not args.arcade:
        args.arcade = os.getenv('ARCADE_NAME')
        if not args.arcade:
            print("Arcade name missing, use -A/--arcade")
            parser.print_help()
            sys.exit(1)
    
    arcade_name = args.arcade
    os.environ["AWS_DEFAULT_REGION"] = common.get_arcade_region(arcade_name)

    tmp_dir = os.getenv("ATMP", '/tmp')
    
    session = boto3.session.Session()
    
    if args.gsd:
        bucket = storage.get_account_global_bucket(session)
    else:
        bucket = storage.get_arcade_buckets(session, arcade_name)['infrastructure']

    gsd_data = storage.load_arcade_json_to_dict(bucket, args.path)
    
    eks_cluster = eks.arcade_to_cluster_name(arcade_name=arcade_name)
    
    if not gsd_data:
        sys.exit(1)
        
    pprint(gsd_data)

    # CHECK EKS STATUS
    eks_status_list = []
    offical_status = get_eks_status(cluster_name=eks_cluster)
    eks_status_list.insert(0, str(offical_status))
    
    while eks_status_list[0] != 'ACTIVE':
        time.sleep(5)
        new_status = get_eks_status(cluster_name=eks_cluster)
        
        if eks_status_list[0] == 'Not Active':
            logging.info('Not Active')
            eks_status_list.insert(0, str(new_status))
            continue
        if eks_status_list[0] == 'CREATING':
            eks_status_list.insert(0, str(new_status))
            logging.info('Creating')
            continue
        if eks_status_list[0] == 'ACTIVE':
            logging.info('Active')
            pprint('EKS is Active')
            break
        if eks_status_list[0] == 'PENDING':
            eks_status_list.insert(0, str(new_status))
            logging.info('PENDING')
            continue
        if eks_status_list[0] == 'UPDATING':
            eks_status_list.insert(0, str(new_status))
            logging.info('UPDATING')
            continue
    
    # Check to see if nodegroups are active 
    nodegroup_status_list = []
    offical_nodegroup_status = get_nodegroup_status(cluster_name=eks_cluster, arcade_name=arcade_name)
    nodegroup_status_list.insert(0, str(offical_nodegroup_status))
    
    while nodegroup_status_list[0] != 'ACTIVE':
        time.sleep(5)
        new_nodegroup_status = get_nodegroup_status(cluster_name=eks_cluster, arcade_name=arcade_name)
        
        if nodegroup_status_list[0] == 'Not Active':
            logging.info('nodegroup is not active')
            nodegroup_status_list.insert(0, str(new_nodegroup_status))
            continue
        if nodegroup_status_list[0] == 'CREATING':
            logging.info('nodegroup is creating')
            nodegroup_status_list.insert(0, str(new_nodegroup_status))
            continue
        if nodegroup_status_list[0] == 'UPDATING':
            logging.info('nodegroup is updating')
            nodegroup_status_list.insert(0, str(new_nodegroup_status))
            continue
        if nodegroup_status_list[0] == 'ACTIVE':

            logging.info('nodegroup is active')
            break
    
    # Adding in Sleep to give AWS/EKS a bit to become ready
    time.sleep(60)
    
    # Check Fluentd Image is present, If not, lets make sure it becomes present.
    if not check_fluentd_present():
        pprint('Fluentd repo/image is not in ECR.')
        sys.exit(1)
        # if not login_to_ecr_and_push_image(arcade_name=arcade_name):
        #     pprint('ERROR: The Fluentd image failed to be created and pushed to ECR.')
        #     logging.info('ERROR: The Fluentd image failed to be created and pushed to ECR.')
        #     sys.exit(1)
    
        # set the context
    k8s.load_arcade_k8s_config(arcade_name)
    
    # time.sleep(60)

    
    # Deploy Fluentd
    if not apply_asteroids_fluentd_daemonset(arcade_name, 'log-relay'):
        pprint('Creation of fluentd node daemon set collector failed')
        logging.info("Creation of fluentd node daemon set collector failed.")
        sys.exit(1)
    

    sys.exit(0) 
    
    
if __name__ == '__main__':
    main()