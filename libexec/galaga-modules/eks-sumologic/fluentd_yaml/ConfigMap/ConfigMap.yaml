# Source: sumologic/templates/setup/custom-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name:  arcade-fluentd-sumologic-setup-custom
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-weight: "2"
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
  labels:
    app: arcade-fluentd-sumologic
    chart: "sumologic-2.18.1"
    release: "arcade-fluentd"
    heritage: "Helm"
data:

---

# Source: sumologic/charts/fluent-bit/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: arcade-fluentd-fluent-bit
  labels:
    helm.sh/chart: fluent-bit-0.20.2
    app.kubernetes.io/name: fluent-bit
    app.kubernetes.io/instance: arcade-fluentd
    app.kubernetes.io/version: "1.9.4"
    app.kubernetes.io/managed-by: Helm
data:
  custom_parsers.conf: |
    [PARSER]
        Name        multi_line
        Format      regex
        Regex       (?<log>^{"log":"\[?\d{4}-\d{1,2}-\d{1,2}.\d{2}:\d{2}:\d{2}.*)
    [PARSER]
        Name         crio
        Format       regex
        Regex        ^(?<time>[^ ]+) (?<stream>stdout|stderr) (?<logtag>[^ ]*) (?<log>.*)$
        Time_Key     time
        Time_Format  %Y-%m-%dT%H:%M:%S.%L%z
    [PARSER]
        Name         containerd
        Format       regex
        Regex        ^(?<time>[^ ]+) (?<stream>stdout|stderr) (?<logtag>[^ ]*) (?<log>.*)$
        Time_Key     time
        Time_Format  %Y-%m-%dT%H:%M:%S.%LZ
    
  fluent-bit.conf: |
    [SERVICE]
        Flush        1
        Daemon       Off
        Log_Level    info
        Parsers_File parsers.conf
        Parsers_File custom_parsers.conf
        HTTP_Server  On
        HTTP_Listen  0.0.0.0
        HTTP_Port    2020
    
    [INPUT]
        Name                tail
        Path                /var/log/containers/*.log
        Docker_Mode         On
        Docker_Mode_Parser  multi_line
        Tag                 containers.*
        Refresh_Interval    1
        Rotate_Wait         60
        Mem_Buf_Limit       5MB
        Skip_Long_Lines     On
        DB                  /tail-db/tail-containers-state-sumo.db
        DB.Sync             Normal
    [INPUT]
        Name            systemd
        Tag             host.*
        DB              /tail-db/systemd-state-sumo.db
        Systemd_Filter  _SYSTEMD_UNIT=addon-config.service
        Systemd_Filter  _SYSTEMD_UNIT=addon-run.service
        Systemd_Filter  _SYSTEMD_UNIT=cfn-etcd-environment.service
        Systemd_Filter  _SYSTEMD_UNIT=cfn-signal.service
        Systemd_Filter  _SYSTEMD_UNIT=clean-ca-certificates.service
        Systemd_Filter  _SYSTEMD_UNIT=containerd.service
        Systemd_Filter  _SYSTEMD_UNIT=coreos-metadata.service
        Systemd_Filter  _SYSTEMD_UNIT=coreos-setup-environment.service
        Systemd_Filter  _SYSTEMD_UNIT=coreos-tmpfiles.service
        Systemd_Filter  _SYSTEMD_UNIT=dbus.service
        Systemd_Filter  _SYSTEMD_UNIT=docker.service
        Systemd_Filter  _SYSTEMD_UNIT=efs.service
        Systemd_Filter  _SYSTEMD_UNIT=etcd-member.service
        Systemd_Filter  _SYSTEMD_UNIT=etcd.service
        Systemd_Filter  _SYSTEMD_UNIT=etcd2.service
        Systemd_Filter  _SYSTEMD_UNIT=etcd3.service
        Systemd_Filter  _SYSTEMD_UNIT=etcdadm-check.service
        Systemd_Filter  _SYSTEMD_UNIT=etcdadm-reconfigure.service
        Systemd_Filter  _SYSTEMD_UNIT=etcdadm-save.service
        Systemd_Filter  _SYSTEMD_UNIT=etcdadm-update-status.service
        Systemd_Filter  _SYSTEMD_UNIT=flanneld.service
        Systemd_Filter  _SYSTEMD_UNIT=format-etcd2-volume.service
        Systemd_Filter  _SYSTEMD_UNIT=kube-node-taint-and-uncordon.service
        Systemd_Filter  _SYSTEMD_UNIT=kubelet.service
        Systemd_Filter  _SYSTEMD_UNIT=ldconfig.service
        Systemd_Filter  _SYSTEMD_UNIT=locksmithd.service
        Systemd_Filter  _SYSTEMD_UNIT=logrotate.service
        Systemd_Filter  _SYSTEMD_UNIT=lvm2-monitor.service
        Systemd_Filter  _SYSTEMD_UNIT=mdmon.service
        Systemd_Filter  _SYSTEMD_UNIT=nfs-idmapd.service
        Systemd_Filter  _SYSTEMD_UNIT=nfs-mountd.service
        Systemd_Filter  _SYSTEMD_UNIT=nfs-server.service
        Systemd_Filter  _SYSTEMD_UNIT=nfs-utils.service
        Systemd_Filter  _SYSTEMD_UNIT=node-problem-detector.service
        Systemd_Filter  _SYSTEMD_UNIT=ntp.service
        Systemd_Filter  _SYSTEMD_UNIT=oem-cloudinit.service
        Systemd_Filter  _SYSTEMD_UNIT=rkt-gc.service
        Systemd_Filter  _SYSTEMD_UNIT=rkt-metadata.service
        Systemd_Filter  _SYSTEMD_UNIT=rpc-idmapd.service
        Systemd_Filter  _SYSTEMD_UNIT=rpc-mountd.service
        Systemd_Filter  _SYSTEMD_UNIT=rpc-statd.service
        Systemd_Filter  _SYSTEMD_UNIT=rpcbind.service
        Systemd_Filter  _SYSTEMD_UNIT=set-aws-environment.service
        Systemd_Filter  _SYSTEMD_UNIT=system-cloudinit.service
        Systemd_Filter  _SYSTEMD_UNIT=systemd-timesyncd.service
        Systemd_Filter  _SYSTEMD_UNIT=update-ca-certificates.service
        Systemd_Filter  _SYSTEMD_UNIT=user-cloudinit.service
        Systemd_Filter  _SYSTEMD_UNIT=var-lib-etcd2.service
        Max_Entries     1000
        Read_From_Tail  true
    
    [FILTER]
        Name kubernetes
        Match kube.*
        Merge_Log On
        Keep_Log Off
        K8S-Logging.Parser On
        K8S-Logging.Exclude On
    
    [OUTPUT]
        Name          forward
        Match         *
        Host          ${FLUENTD_LOGS_SVC}.${NAMESPACE}.svc.cluster.local.
        Port          24321
        Retry_Limit   False
        tls           off
        tls.verify    on
        tls.debug     1
        # Disable keepalive for better load balancing
        net.keepalive off
---

apiVersion: v1
kind: ConfigMap
metadata:
  name: sumologic-configmap
data:
  fluentdLogs: arcade-fluentd-sumologic-fluentd-logs
  fluentdMetrics: arcade-fluentd-sumologic-fluentd-metrics
  fluentdNamespace: fluentd

---

apiVersion: v1
kind: ConfigMap
metadata:
  name: arcade-fluentd-sumologic-fluentd-logs
  labels:
    app: arcade-fluentd-sumologic-fluentd-logs
    chart: "sumologic-2.18.1"
    release: "arcade-fluentd"
    heritage: "Helm"
data:
  fluent.conf: |-
    @include common.conf
    @include logs.conf
  buffer.output.conf: |
    compress "gzip"
    flush_interval "5s"
    flush_thread_count "8"
    chunk_limit_size "1m"
    total_limit_size "128m"
    queued_chunks_limit_size "128"
    overflow_action drop_oldest_chunk
    retry_max_interval "10m"
    retry_forever "true"
  common.conf: |-
    # Prevent fluentd from handling records containing its own logs and health checks.
    <match fluentd.pod.healthcheck>
      @type relabel
      @label @FLUENT_LOG
    </match>
    <label @FLUENT_LOG>
      <match **>
        @type null
      </match>
    </label>
    # expose the Fluentd metrics to Prometheus
    <source>
      @type prometheus
      metrics_path /metrics
      port 24231
    </source>
    <source>
      @type prometheus_output_monitor
    </source>
    <source>
      @type http
      port 9880
      bind 0.0.0.0
    </source>
    <system>
      log_level info
    </system>
  logs.conf: |
    <source>
      @type forward
      port 24321
      bind 0.0.0.0
      
    </source>
    @include logs.source.containers.conf
    @include logs.source.systemd.conf
    @include logs.source.default.conf
  logs.enhance.k8s.metadata.filter.conf: |
    cache_size  "10000"
    cache_ttl  "7200"
    cache_refresh "3600"
    cache_refresh_variation "900"
    in_namespace_path '$.kubernetes.namespace_name'
    in_pod_path '$.kubernetes.pod_name'
    core_api_versions v1
    api_groups apps/v1
    data_type logs
    add_owners true
    add_service true
    cache_refresh_apiserver_request_delay "0"
    cache_refresh_exclude_pod_regex ""
  logs.kubernetes.metadata.filter.conf: |
    annotation_match ["sumologic\.com.*"]
    de_dot false
    watch "true"
    ca_file ""
    verify_ssl "true"
    client_cert ""
    client_key ""
    bearer_token_file ""
    cache_size "10000"
    cache_ttl "7200"
    tag_to_kubernetes_name_regexp ".+?\\.containers\\.(?<pod_name>[^_]+)_(?<namespace>[^_]+)_(?<container_name>.+)-(?<docker_id>[a-z0-9]{64})\\.log$"
  logs.kubernetes.sumologic.filter.conf: |
    source_name "%{{namespace}}.%{{pod}}.%{{container}}"
    source_host 
    log_format "fields"
    source_category "%{{namespace}}/%{{pod_name}}"
    source_category_prefix "kubernetes/"
    source_category_replace_dash "/"
    exclude_pod_regex ""
    exclude_container_regex ""
    exclude_host_regex ""
    per_container_annotations_enabled false
    per_container_annotation_prefixes 
  logs.output.conf: |
    data_type logs
    log_key log
    endpoint "#{ENV['SUMO_ENDPOINT_DEFAULT_LOGS_SOURCE']}"
    verify_ssl "true"
    log_format "fields"
    add_timestamp "true"
    timestamp_key "timestamp"
    proxy_uri ""
    compress "true"
    compress_encoding "gzip"
  logs.source.containers.conf: |
    
    
    <filter containers.**>
      @type record_transformer
      enable_ruby
      renew_record true
      <record>
        log    ${{record["log"].split(/[\\n\\t]+/).map! {{|item| JSON.parse(item)["log"]}}.any? ? record["log"].split(/[\\n\\t]+/).map! {{|item| JSON.parse(item)["log"]}}.join("") : record["log"] rescue record["log"]}}
        stream ${{[record["log"].split(/[\\n\\t]+/)[0]].map! {{|item| JSON.parse(item)["stream"]}}.any? ? [record["log"].split(/[\\n\\t]+/)[0]].map! {{|item| JSON.parse(item)["stream"]}}.join("") : record["stream"] rescue record["stream"]}}
        time   ${{[record["log"].split(/[\\n\\t]+/)[0]].map! {{|item| JSON.parse(item)["time"]}}.any? ? [record["log"].split(/[\\n\\t]+/)[0]].map! {{|item| JSON.parse(item)["time"]}}.join("") : record["time"] rescue record["time"]}}
      </record>
    </filter>
    # match all  container logs and label them @NORMAL
    <match containers.**>
      @type relabel
      @label @NORMAL
    </match>
    <label @NORMAL>
    
      # only match fluentd logs based on fluentd container log file name.
      # by default, this is <filter **collection-sumologic-fluentd**>
      <filter **arcade-fluentd-sumologic-fluentd**>
        # only ingest:
        #   - stacktraces (containing /usr/local)
        #   - fluentd logs of levels: {{error, fatal}}: `\[error\]|\[fatal\]`
        #   - warning messages if buffer is full `drop_oldest_chunk|retry succeeded`
        @type grep
        <regexp>
          key log
          pattern /\/usr\/local|\[error\]|\[fatal\]|drop_oldest_chunk|retry succeeded/
        </regexp>
      </filter>
  
    
  
      # third-party Kubernetes metadata  filter plugin
      <filter containers.**>
        @type kubernetes_metadata
        @log_level error
        @include logs.kubernetes.metadata.filter.conf
      </filter>
      # Sumo Logic Kubernetes metadata enrichment filter plugin
      <filter containers.**>
        @type enhance_k8s_metadata
        @log_level error
        @include logs.enhance.k8s.metadata.filter.conf
      </filter>
      
      # Kubernetes Sumo Logic filter plugin
      <filter containers.**>
        @type kubernetes_sumologic
        @include logs.kubernetes.sumologic.filter.conf
        exclude_namespace_regex ""
      </filter>
      <filter **>
        @type record_modifier
        <record>
          _sumo_metadata ${{record["_sumo_metadata"][:fields] = record["_sumo_metadata"].fetch(:fields, "").split(",").append("cluster={eks_cluster}").join(","); record["_sumo_metadata"]}}
        </record>
      </filter>
      
      
      <match containers.**>
        @type copy
        <store>
          @type sumologic
          @id sumologic.endpoint.logs
          sumo_client "k8s_2.18.1"
          @log_level error
          @include logs.output.conf
          <buffer>
            @type file
            path /fluentd/buffer/logs.containers
            @include buffer.output.conf
          </buffer>
        </store>
      </match>
    </label>
  logs.source.default.conf: |
    
    <filter **>
      @type grep
      <exclude>
        key message
        pattern /disable filter chain optimization/
      </exclude>
    </filter>
      
    <filter **>
      @type kubernetes_sumologic
      source_name "k8s_default"
      source_category "default"
      source_category_prefix "kubernetes/"
      source_category_replace_dash "/"
      exclude_facility_regex ""
      exclude_host_regex ""
      exclude_priority_regex ""
      exclude_unit_regex ""
    </filter>
    <filter **>
      @type record_modifier
      <record>
        _sumo_metadata ${{record["_sumo_metadata"][:fields] = record["_sumo_metadata"].fetch(:fields, "").split(",").append("cluster={eks_cluster}").join(","); record["_sumo_metadata"]}}
      </record>
    </filter>
      
    <match **>
      @type copy
      <store>
        @type sumologic
        @id sumologic.endpoint.logs.default
        sumo_client "k8s_2.18.1"
        @include logs.output.conf
        <buffer>
          @type file
          path /fluentd/buffer/logs.default
          @include buffer.output.conf
        </buffer>
      </store>
    </match>
  logs.source.systemd.conf: |
    
    <match host.kubelet.**>
      @type relabel
      @label @KUBELET
    </match>
    <label @KUBELET>
      
      <filter host.kubelet.**>
        @type kubernetes_sumologic
        source_category "kubelet"
        source_name "k8s_kubelet"
        source_category_prefix "kubernetes/"
        source_category_replace_dash "/"
        exclude_facility_regex ""
        exclude_host_regex ""
        exclude_priority_regex ""
        exclude_unit_regex ""
      </filter>
      <filter **>
        @type record_modifier
        <record>
          _sumo_metadata ${{record["_sumo_metadata"][:fields] = record["_sumo_metadata"].fetch(:fields, "").split(",").append("cluster={eks_cluster}").join(","); record["_sumo_metadata"]}}
        </record>
      </filter>
      
      
      <match **>
        @type copy
        <store>
          @type sumologic
          @id sumologic.endpoint.logs.kubelet
          sumo_client "k8s_2.18.1"
          @include logs.output.conf
          <buffer>
            @type file
            path /fluentd/buffer/logs.kubelet
            @include buffer.output.conf
          </buffer>
        </store>
      </match>
    </label>
    
    <match host.**>
      @type relabel
      @label @SYSTEMD
    </match>
    <label @SYSTEMD>
      
      <filter host.**>
        @type kubernetes_sumologic
        source_name "k8s_systemd"
        source_category "system"
        source_category_prefix "kubernetes/"
        source_category_replace_dash "/"
        exclude_facility_regex ""
        exclude_host_regex ""
        exclude_priority_regex ""
        exclude_unit_regex ""
      </filter>
      <filter host.**>
        @type record_modifier
        <record>
          _sumo_metadata ${{record["_sumo_metadata"][:source] = tag_parts[1]; record["_sumo_metadata"]}}
        </record>
      </filter>
      <filter **>
        @type record_modifier
        <record>
          _sumo_metadata ${{record["_sumo_metadata"][:fields] = record["_sumo_metadata"].fetch(:fields, "").split(",").append("cluster={eks_cluster}").join(","); record["_sumo_metadata"]}}
        </record>
      </filter>
      
      
      <match **>
        @type copy
        <store>
          @type sumologic
          @id sumologic.endpoint.logs.systemd
          sumo_client "k8s_2.18.1"
          @include logs.output.conf
          <buffer>
            @type file
            path /fluentd/buffer/logs.systemd
            @include buffer.output.conf
          </buffer>
        </store>
      </match>
    </label>

---

apiVersion: v1
kind: ConfigMap
metadata:
  name: arcade-fluentd-sumologic-fluentd-events
  labels:
    app: arcade-fluentd-sumologic-fluentd-events
    chart: "sumologic-2.18.1"
    release: "arcade-fluentd"
    heritage: "Helm"
data:
  fluent.conf: |-
    @include events.conf
  events.conf: |
    <source>
      @type events
      deploy_namespace fluentd
    </source>
    # Prevent fluentd from handling records containing its own logs.
    <match fluentd.**>
      @type null
    </match>
    # Set `cluster` metadata field
    <filter **>
      @type record_modifier
      <record>
        _sumo_metadata ${{ {{:fields => "cluster={eks_cluster}"}} }}
      </record>
    </filter>
    # expose the Fluentd metrics to Prometheus
    <source>
      @type prometheus
      metrics_path /metrics
      port 24231
    </source>
    <source>
      @type prometheus_output_monitor
    </source>
    <source>
      @type http
      port 9880
      bind 0.0.0.0
    </source>
    <match kubernetes.**>
      @type copy
      <store>
        @type sumologic
        @id sumologic.endpoint.events
        sumo_client "k8s_2.18.1"
        endpoint "#{{ENV['SUMO_ENDPOINT_DEFAULT_EVENTS_SOURCE']}}"
        source_name events
        source_category {eks_cluster}/events
        data_type logs
        disable_cookies true
        verify_ssl "true"
        proxy_uri ""
        compress "true"
        compress_encoding "gzip"
        <buffer>
          @type file
          path /fluentd/buffer/events
          @include buffer.output.conf
        </buffer>
      </store>
    </match>
    <system>
      log_level info
    </system>
  buffer.output.conf: |
    compress "gzip"
    flush_interval "5s"
    flush_thread_count "8"
    chunk_limit_size "1m"
    total_limit_size "128m"
    queued_chunks_limit_size "128"
    overflow_action drop_oldest_chunk
    retry_max_interval "10m"
    retry_forever "true"


---
apiVersion: v1
kind: ConfigMap
metadata:
  name: arcade-fluentd-sumologic-fluentd-metrics
  labels:
    app: arcade-fluentd-sumologic-fluentd-metrics
    chart: "sumologic-2.18.1"
    release: "arcade-fluentd"
    heritage: "Helm"
data:
  fluent.conf: |-
    @include common.conf
    @include metrics.conf
  buffer.output.conf: |
        compress "gzip"
        flush_interval "5s"
        flush_thread_count "8"
        chunk_limit_size "1m"
        total_limit_size "128m"
        queued_chunks_limit_size "128"
        overflow_action drop_oldest_chunk
        retry_max_interval "10m"
        retry_forever "true"
  common.conf: |-
    # Prevent fluentd from handling records containing its own logs and health checks.
    <match fluentd.pod.healthcheck>
      @type relabel
      @label @FLUENT_LOG
    </match>
    <label @FLUENT_LOG>
      <match **>
        @type null
      </match>
    </label>
    # expose the Fluentd metrics to Prometheus
    <source>
      @type prometheus
      metrics_path /metrics
      port 24231
    </source>
    <source>
      @type prometheus_output_monitor
    </source>
    <source>
      @type http
      port 9880
      bind 0.0.0.0
    </source>
    <system>
      log_level info
    </system>
      metrics.conf: |
        <source>
          @type http
          port 9888
          <parse>
            @type protobuf
          </parse>
        </source>
        <match prometheus.metrics**>
          @type datapoint
          @label @DATAPOINT
        </match>
        <label @DATAPOINT>
          <filter prometheus.metrics**>
            @type record_modifier
            <record>
              cluster {eks_cluster}
            </record>
          </filter>
          <filter prometheus.metrics**>
            @type enhance_k8s_metadata
            cache_size  "10000"
            cache_ttl  "7200"
            cache_refresh "3600"
            cache_refresh_variation "900"
            cache_refresh_apiserver_request_delay "0"
            cache_refresh_exclude_pod_regex ""
            core_api_versions v1
            api_groups apps/v1
            add_owners true
            add_service true
          </filter>

          <filter prometheus.metrics**>
            @type prometheus_format
            relabel container_name:container,pod_name:pod
          </filter>

          <match prometheus.metrics.apiserver**>
            @type copy
            <store>
              @type sumologic
              @id sumologic.endpoint.metrics.apiserver
              sumo_client "k8s_2.18.1"
              endpoint "#{{ENV['SUMO_ENDPOINT_APISERVER_METRICS_SOURCE']}}"
            @include metrics.output.conf
              <buffer>
                @type file
                path /fluentd/buffer/metrics.apiserver
                @include buffer.output.conf
              </buffer>
            </store>
          </match>

          <match prometheus.metrics.container**>
            @type copy
            <store>
              @type sumologic
              @id sumologic.endpoint.metrics.container
              sumo_client "k8s_2.18.1"
              endpoint "#{{ENV['SUMO_ENDPOINT_KUBELET_METRICS_SOURCE']}}"
            @include metrics.output.conf
              <buffer>
                @type file
                path /fluentd/buffer/metrics.container
                @include buffer.output.conf
              </buffer>
            </store>
          </match>

          <match prometheus.metrics.control-plane**>
            @type copy
            <store>
              @type sumologic
              @id sumologic.endpoint.metrics.control.plane
              sumo_client "k8s_2.18.1"
              endpoint "#{{ENV['SUMO_ENDPOINT_CONTROL_PLANE_METRICS_SOURCE']}}"
            @include metrics.output.conf
              <buffer>
                @type file
                path /fluentd/buffer/metrics.control_plane
                @include buffer.output.conf
              </buffer>
            </store>
          </match>

          <match prometheus.metrics.controller-manager**>
            @type copy
            <store>
              @type sumologic
              @id sumologic.endpoint.metrics.kube.controller.manager
              sumo_client "k8s_2.18.1"
              endpoint "#{{ENV['SUMO_ENDPOINT_CONTROLLER_METRICS_SOURCE']}}"
            @include metrics.output.conf
              <buffer>
                @type file
                path /fluentd/buffer/metrics.controller
                @include buffer.output.conf
              </buffer>
            </store>
          </match>

          <match prometheus.metrics.kubelet**>
            @type copy
            <store>
              @type sumologic
              @id sumologic.endpoint.metrics.kubelet
              sumo_client "k8s_2.18.1"
              endpoint "#{{ENV['SUMO_ENDPOINT_KUBELET_METRICS_SOURCE']}}"
            @include metrics.output.conf
              <buffer>
                @type file
                path /fluentd/buffer/metrics.kubelet
                @include buffer.output.conf
              </buffer>
            </store>
          </match>

          <match prometheus.metrics.node**>
            @type copy
            <store>
              @type sumologic
              @id sumologic.endpoint.metrics.node.exporter
              sumo_client "k8s_2.18.1"
              endpoint "#{{ENV['SUMO_ENDPOINT_NODE_METRICS_SOURCE']}}"
            @include metrics.output.conf
              <buffer>
                @type file
                path /fluentd/buffer/metrics.node
                @include buffer.output.conf
              </buffer>
            </store>
          </match>

          <match prometheus.metrics.scheduler**>
            @type copy
            <store>
              @type sumologic
              @id sumologic.endpoint.metrics.kube.scheduler
              sumo_client "k8s_2.18.1"
              endpoint "#{{ENV['SUMO_ENDPOINT_SCHEDULER_METRICS_SOURCE']}}"
            @include metrics.output.conf
              <buffer>
                @type file
                path /fluentd/buffer/metrics.scheduler
                @include buffer.output.conf
              </buffer>
            </store>
          </match>

          <match prometheus.metrics.state**>
            @type copy
            <store>
              @type sumologic
              @id sumologic.endpoint.metrics.kube.state
              sumo_client "k8s_2.18.1"
              endpoint "#{{ENV['SUMO_ENDPOINT_STATE_METRICS_SOURCE']}}"
            @include metrics.output.conf
              <buffer>
                @type file
                path /fluentd/buffer/metrics.state
                @include buffer.output.conf
              </buffer>
            </store>
          </match>

          <match prometheus.metrics**>
            @type copy
            <store>
              @type sumologic
              @id sumologic.endpoint.metrics
              sumo_client "k8s_2.18.1"
              endpoint "#{{ENV['SUMO_ENDPOINT_DEFAULT_METRICS_SOURCE']}}"
            @include metrics.output.conf
              <buffer>
                @type file
                path /fluentd/buffer/metrics.default
                @include buffer.output.conf
              </buffer>
            </store>
          </match>

        </label>
      metrics.output.conf: |
        data_type metrics
        metric_data_format prometheus
        disable_cookies true
        proxy_uri ""
        compress "true"
        compress_encoding "gzip"


---

apiVersion: v1
kind: ConfigMap
metadata:
  name:  arcade-fluentd-sumologic-setup
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-weight: "2"
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
  labels:
    app: arcade-fluentd-sumologic
    chart: "sumologic-2.18.1"
    release: "arcade-fluentd"
    heritage: "Helm"
data:
  custom.sh: |
    #!/bin/bash
    #
    # This script copies files from /customer-scripts to /scripts/<dirname> basing on the filename
    #
    # Example file structure:
    #
    # /customer-scripts
    # ├── dir1_main.tf
    # ├── dir1_setup.sh
    # ├── dir2_list.txt
    # └── dir2_setup.sh
    #
    # Expected structure:
    #
    # /scripts
    # ├── dir1
    # │   ├── main.tf
    # │   └── setup.sh
    # └── dir2
    #     ├── list.txt
    #     └── setup.sh
    #
    # shellcheck disable=SC2010
    # extract target directory names from the file names using _ as separator
    err_report() {
        echo "Custom script error on line $1"
        exit 1
    }
    trap 'err_report $LINENO' ERR
  
    for dir in $(ls -1 /customer-scripts | grep _ | grep -oE '^.*?_' | sed 's/_//g' | sort | uniq); do
      target="/scripts/${dir}"
      mkdir "${target}"
      # shellcheck disable=SC2010
      # Get files for given directory and take only filename part (after first _)
      for file in $(ls -1 "/customer-scripts/${dir}_"* | grep -oE '_.*' | sed 's/_//g'); do
        cp "/customer-scripts/${dir}_${file}" "${target}/${file}"
      done
  
      if [[ ! -f setup.sh ]]; then
        echo "You're missing setup.sh script in custom scripts directory: '${dir}'"
        continue
      fi
  
      cd "${target}" && bash setup.sh
    done
  dashboards.sh: |
    #!/bin/bash
  
    SUMOLOGIC_ACCESSID=${SUMOLOGIC_ACCESSID:=""}
    readonly SUMOLOGIC_ACCESSID
    SUMOLOGIC_ACCESSKEY=${SUMOLOGIC_ACCESSKEY:=""}
    readonly SUMOLOGIC_ACCESSKEY
    SUMOLOGIC_BASE_URL=${SUMOLOGIC_BASE_URL:=""}
    readonly SUMOLOGIC_BASE_URL
  
    INTEGRATIONS_FOLDER_NAME="Sumo Logic Integrations"
    K8S_FOLDER_NAME="Kubernetes"
    K8S_APP_UUID="162ceac7-166a-4475-8427-65e170ae9837"
  
    function load_dashboards_folder_id() {
      local ADMIN_FOLDER_JOB_ID
      ADMIN_FOLDER_JOB_ID="$(curl -XGET -s \
              -u "${SUMOLOGIC_ACCESSID}:${SUMOLOGIC_ACCESSKEY}" \
              -H "isAdminMode: true" \
              "${SUMOLOGIC_BASE_URL}"v2/content/folders/adminRecommended | jq '.id' | tr -d '"' )"
      readonly ADMIN_FOLDER_JOB_ID
  
      local ADMIN_FOLDER_JOB_STATUS
      ADMIN_FOLDER_JOB_STATUS="InProgress"
      while [ "${ADMIN_FOLDER_JOB_STATUS}" = "InProgress" ]; do
        ADMIN_FOLDER_JOB_STATUS="$(curl -XGET -s \
              -u "${SUMOLOGIC_ACCESSID}:${SUMOLOGIC_ACCESSKEY}" \
              "${SUMOLOGIC_BASE_URL}"v2/content/folders/adminRecommended/"${ADMIN_FOLDER_JOB_ID}"/status | jq '.status' | tr -d '"' )"
  
        sleep 1
      done
  
      if [ "${ADMIN_FOLDER_JOB_STATUS}" != "Success" ]; then
        echo "Could not fetch data from the \"Admin Recommended\" content folder. The K8s Dashboards won't be installed."
        echo "You can still install them manually:"
        echo "https://help.sumologic.com/docs/integrations/containers-orchestration/kubernetes#installing-the-kubernetes-app"
        exit 1
      fi
  
      local ADMIN_FOLDER
      ADMIN_FOLDER="$(curl -XGET -s \
              -u "${SUMOLOGIC_ACCESSID}:${SUMOLOGIC_ACCESSKEY}" \
              -H "isAdminMode: true" \
              "${SUMOLOGIC_BASE_URL}"v2/content/folders/adminRecommended/"${ADMIN_FOLDER_JOB_ID}"/result )"
      readonly ADMIN_FOLDER
  
      local ADMIN_FOLDER_CHILDREN
      ADMIN_FOLDER_CHILDREN="$( echo "${ADMIN_FOLDER}" | jq '.children[]')"
      readonly ADMIN_FOLDER_CHILDREN
  
      local ADMIN_FOLDER_ID
      ADMIN_FOLDER_ID="$( echo "${ADMIN_FOLDER}" | jq '.id' | tr -d '"')"
      readonly ADMIN_FOLDER_ID
  
      INTEGRATIONS_FOLDER_ID="$( echo "${ADMIN_FOLDER_CHILDREN}" | \
                jq -r "select(.name == \"${INTEGRATIONS_FOLDER_NAME}\") | .id" )"
  
      if [[ -z "${INTEGRATIONS_FOLDER_ID}" ]]; then
        INTEGRATIONS_FOLDER_ID="$(curl -XPOST -s \
                -u "${SUMOLOGIC_ACCESSID}:${SUMOLOGIC_ACCESSKEY}" \
                -H "isAdminMode: true" \
                -H "Content-Type: application/json" \
                -d "{\"name\":\"${INTEGRATIONS_FOLDER_NAME}\",\"parentId\":\"${ADMIN_FOLDER_ID}\",\"description\":\"Content provided by the Sumo Logic integrations.\"}" \
                "${SUMOLOGIC_BASE_URL}"v2/content/folders | \
                jq -r " .id" )"
      fi
  
      local INTEGRATIONS_FOLDER_CHILDREN
      INTEGRATIONS_FOLDER_CHILDREN="$(curl -XGET -s \
                -u "${SUMOLOGIC_ACCESSID}:${SUMOLOGIC_ACCESSKEY}" \
                -H "isAdminMode: true" \
                "${SUMOLOGIC_BASE_URL}"v2/content/folders/"${INTEGRATIONS_FOLDER_ID}" | \
                jq '.children[]')"
      readonly INTEGRATIONS_FOLDER_CHILDREN
  
      K8S_FOLDER_ID="$( echo "${INTEGRATIONS_FOLDER_CHILDREN}" | \
                jq -r "select(.name == \"${K8S_FOLDER_NAME}\") | .id" )"
    }
  
    load_dashboards_folder_id
  
    if [[ -z "${K8S_FOLDER_ID}" ]]; then
      APP_INSTALL_JOB_RESPONSE="$(curl -XPOST -s \
             -u "${SUMOLOGIC_ACCESSID}:${SUMOLOGIC_ACCESSKEY}" \
             -H "isAdminMode: true" \
             -H "Content-Type: application/json" \
             -d "{\"name\":\"${K8S_FOLDER_NAME}\",\"destinationFolderId\":\"${INTEGRATIONS_FOLDER_ID}\",\"description\":\"Kubernetes dashboards provided by Sumo Logic.\"}" \
             "${SUMOLOGIC_BASE_URL}"v1/apps/"${K8S_APP_UUID}"/install )"
      readonly APP_INSTALL_JOB_RESPONSE
  
      APP_INSTALL_JOB_ID="$(echo "${APP_INSTALL_JOB_RESPONSE}" | jq '.id' | tr -d '"' )"
      readonly APP_INSTALL_JOB_ID
  
      APP_INSTALL_JOB_STATUS="InProgress"
      while [ "${APP_INSTALL_JOB_STATUS}" = "InProgress" ]; do
        APP_INSTALL_JOB_STATUS="$(curl -XGET -s \
              -u "${SUMOLOGIC_ACCESSID}:${SUMOLOGIC_ACCESSKEY}" \
              "${SUMOLOGIC_BASE_URL}"v1/apps/install/"${APP_INSTALL_JOB_ID}"/status | jq '.status' | tr -d '"' )"
  
        sleep 1
      done
  
      if [ "${APP_INSTALL_JOB_STATUS}" != "Success" ]; then
        ERROR_MSG="$(curl -XGET -s \
              -u "${SUMOLOGIC_ACCESSID}:${SUMOLOGIC_ACCESSKEY}" \
              "${SUMOLOGIC_BASE_URL}"v1/apps/install/"${APP_INSTALL_JOB_ID}"/status )"
        echo "${ERROR_MSG}"
  
        echo "Installation of the K8s Dashboards failed."
        echo "You can still install them manually:"
        echo "https://help.sumologic.com/docs/integrations/containers-orchestration/kubernetes#installing-the-kubernetes-app"
        exit 2
      else
        load_dashboards_folder_id
  
        ORG_ID="$(curl -XGET -s \
                -u "${SUMOLOGIC_ACCESSID}:${SUMOLOGIC_ACCESSKEY}" \
                "${SUMOLOGIC_BASE_URL}"v1/account/contract | jq '.orgId' | tr -d '"' )"
        readonly ORG_ID
  
        PERMS_ERRORS=$( curl -XPUT -s \
          -u "${SUMOLOGIC_ACCESSID}:${SUMOLOGIC_ACCESSKEY}" \
          -H "isAdminMode: true" \
          -H "Content-Type: application/json" \
          -d "{\"contentPermissionAssignments\": [{\"permissionName\": \"View\",\"sourceType\": \"org\",\"sourceId\": \"${ORG_ID}\",\"contentId\": \"${K8S_FOLDER_ID}\"}],\"notifyRecipients\":false,\"notificationMessage\":\"\"}" \
          "${SUMOLOGIC_BASE_URL}"v2/content/"${K8S_FOLDER_ID}"/permissions/add | jq '.errors' )
        readonly PERMS_ERRORS
  
        if [ "${PERMS_ERRORS}" != "null" ]; then
          echo "Setting permissions for the installed content failed."
          echo "${PERMS_ERRORS}"
        fi
  
        echo "Installation of the K8s Dashboards succeeded."
      fi
    else
      echo "The K8s Dashboards have been already installed."
      echo "You can (re)install them manually with:"
      echo "https://help.sumologic.com/docs/integrations/containers-orchestration/kubernetes#installing-the-kubernetes-app"
    fi
  fields.tf: |
    resource "sumologic_field" "cluster" {
      count = var.create_fields ? 1 : 0
  
      field_name = "cluster"
      data_type = "String"
      state = "Enabled"
    }
    resource "sumologic_field" "container" {
      count = var.create_fields ? 1 : 0
  
      field_name = "container"
      data_type = "String"
      state = "Enabled"
    }
    resource "sumologic_field" "deployment" {
      count = var.create_fields ? 1 : 0
  
      field_name = "deployment"
      data_type = "String"
      state = "Enabled"
    }
    resource "sumologic_field" "host" {
      count = var.create_fields ? 1 : 0
  
      field_name = "host"
      data_type = "String"
      state = "Enabled"
    }
    resource "sumologic_field" "namespace" {
      count = var.create_fields ? 1 : 0
  
      field_name = "namespace"
      data_type = "String"
      state = "Enabled"
    }
    resource "sumologic_field" "node" {
      count = var.create_fields ? 1 : 0
  
      field_name = "node"
      data_type = "String"
      state = "Enabled"
    }
    resource "sumologic_field" "pod" {
      count = var.create_fields ? 1 : 0
  
      field_name = "pod"
      data_type = "String"
      state = "Enabled"
    }
    resource "sumologic_field" "service" {
      count = var.create_fields ? 1 : 0
  
      field_name = "service"
      data_type = "String"
      state = "Enabled"
    }
  locals.tf: |
    locals {
      default_events_source                       = "events"
      default_logs_source                         = "logs"
      apiserver_metrics_source                    = "apiserver-metrics"
      control_plane_metrics_source                = "control-plane-metrics"
      controller_metrics_source                   = "kube-controller-manager-metrics"
      default_metrics_source                      = "(default-metrics)"
      kubelet_metrics_source                      = "kubelet-metrics"
      node_metrics_source                         = "node-exporter-metrics"
      scheduler_metrics_source                    = "kube-scheduler-metrics"
      state_metrics_source                        = "kube-state-metrics"
    }
  main.tf: |
    terraform {
      required_providers {
        sumologic = {
          source  = "sumologic/sumologic"
          version = "~> 2.18"
        }
        kubernetes = {
          source  = "hashicorp/kubernetes"
          version = "~> 2.4"
        }
      }
    }
  monitors.sh: |
    #!/bin/bash
  
    SUMOLOGIC_ACCESSID=${SUMOLOGIC_ACCESSID:=""}
    readonly SUMOLOGIC_ACCESSID
    SUMOLOGIC_ACCESSKEY=${SUMOLOGIC_ACCESSKEY:=""}
    readonly SUMOLOGIC_ACCESSKEY
    SUMOLOGIC_BASE_URL=${SUMOLOGIC_BASE_URL:=""}
    readonly SUMOLOGIC_BASE_URL
  
    INTEGRATIONS_FOLDER_NAME="Sumo Logic Integrations"
    MONITORS_FOLDER_NAME="Kubernetes"
    MONITORS_DISABLED="false"
  
    MONITORS_ROOT_ID="$(curl -XGET -s \
            -u "${SUMOLOGIC_ACCESSID}:${SUMOLOGIC_ACCESSKEY}" \
            "${SUMOLOGIC_BASE_URL}"v1/monitors/root | jq -r '.id' )"
    readonly MONITORS_ROOT_ID
  
    # verify if the integrations folder already exists
    INTEGRATIONS_RESPONSE="$(curl -XGET -s -G \
            -u "${SUMOLOGIC_ACCESSID}:${SUMOLOGIC_ACCESSKEY}" \
            "${SUMOLOGIC_BASE_URL}"v1/monitors/search \
            --data-urlencode "query=type:folder ${INTEGRATIONS_FOLDER_NAME}" | \
            jq '.[]' )"
    readonly INTEGRATIONS_RESPONSE
  
    INTEGRATIONS_FOLDER_ID="$( echo "${INTEGRATIONS_RESPONSE}" | \
            jq -r "select(.item.name == \"${INTEGRATIONS_FOLDER_NAME}\") | select(.item.parentId == \"${MONITORS_ROOT_ID}\") | .item.id" )"
  
    # and create it if necessary
    if [[ -z "${INTEGRATIONS_FOLDER_ID}" ]]; then
      INTEGRATIONS_FOLDER_ID="$(curl -XPOST -s \
                  -u "${SUMOLOGIC_ACCESSID}:${SUMOLOGIC_ACCESSKEY}" \
                  -H "Content-Type: application/json" \
                  -d "{\"name\":\"${INTEGRATIONS_FOLDER_NAME}\",\"type\":\"MonitorsLibraryFolder\",\"description\":\"Monitors provided by the Sumo Logic integrations.\"}" \
                  "${SUMOLOGIC_BASE_URL}"v1/monitors?parentId="${MONITORS_ROOT_ID}" | \
                  jq -r " .id" )"
    fi
  
    # verify if the k8s monitors folder already exists
    MONITORS_RESPONSE="$(curl -XGET -s -G \
            -u "${SUMOLOGIC_ACCESSID}:${SUMOLOGIC_ACCESSKEY}" \
            "${SUMOLOGIC_BASE_URL}"v1/monitors/search \
            --data-urlencode "query=type:folder ${MONITORS_FOLDER_NAME}" | \
            jq '.[]' )"
    readonly MONITORS_RESPONSE
  
    MONITORS_FOLDER_ID="$( echo "${MONITORS_RESPONSE}" | \
            jq -r "select(.item.name == \"${MONITORS_FOLDER_NAME}\") | select(.item.parentId == \"${INTEGRATIONS_FOLDER_ID}\") | .item.id" )"
    readonly MONITORS_FOLDER_ID
  
    if [[ -z "${MONITORS_FOLDER_ID}" ]]; then
      # go to monitors directory
      cd /monitors || exit 2
  
      # Fall back to init -upgrade to prevent:
      # Error: Inconsistent dependency lock file
      terraform init -input=false || terraform init -input=false -upgrade
  
      # extract environment from SUMOLOGIC_BASE_URL
      # see: https://help.sumologic.com/docs/api/getting-started/#sumo-logic-endpoints-by-deployment-and-firewall-security
      SUMOLOGIC_ENV=$( echo "${SUMOLOGIC_BASE_URL}" | sed -E 's/https:\/\/.*(au|ca|de|eu|fed|in|jp|us2)\.sumologic\.com.*/\1/' )
      if [[ "${SUMOLOGIC_BASE_URL}" == "${SUMOLOGIC_ENV}" ]] ; then
        SUMOLOGIC_ENV="us1"
      fi
  
      TF_LOG_PROVIDER=DEBUG terraform apply \
          -auto-approve \
          -var="access_id=${SUMOLOGIC_ACCESSID}" \
          -var="access_key=${SUMOLOGIC_ACCESSKEY}" \
          -var="environment=${SUMOLOGIC_ENV}" \
          -var="folder=${MONITORS_FOLDER_NAME}" \
          -var="folder_parent_id=${INTEGRATIONS_FOLDER_ID}" \
          -var="monitors_disabled=${MONITORS_DISABLED}" \
          || { echo "Error during applying Terraform monitors."; exit 1; }
    else
      echo "The monitors have been already installed in ${MONITORS_FOLDER_NAME}."
      echo "You can (re)install them manually with:"
      echo "https://github.com/SumoLogic/terraform-sumologic-sumo-logic-monitor/tree/main/monitor_packages/kubernetes"
    fi
  providers.tf: |-
    provider "sumologic" {}
  
    provider "kubernetes" {
    
        cluster_ca_certificate    = file("/var/run/secrets/kubernetes.io/serviceaccount/ca.crt")
        host                      = "https://kubernetes.default.svc"
        token                     = file("/var/run/secrets/kubernetes.io/serviceaccount/token")
    }
  resources.tf: |
    resource "sumologic_collector" "collector" {
        name  = var.collector_name
        fields  = {
        }
    }
    
    resource "sumologic_http_source" "default_events_source" {
        name         = local.default_events_source
        collector_id = sumologic_collector.collector.id
    }
    
    resource "sumologic_http_source" "default_logs_source" {
        name         = local.default_logs_source
        collector_id = sumologic_collector.collector.id
    }
    
    resource "sumologic_http_source" "apiserver_metrics_source" {
        name         = local.apiserver_metrics_source
        collector_id = sumologic_collector.collector.id
    }
    
    resource "sumologic_http_source" "control_plane_metrics_source" {
        name         = local.control_plane_metrics_source
        collector_id = sumologic_collector.collector.id
    }
    
    resource "sumologic_http_source" "controller_metrics_source" {
        name         = local.controller_metrics_source
        collector_id = sumologic_collector.collector.id
    }
    
    resource "sumologic_http_source" "default_metrics_source" {
        name         = local.default_metrics_source
        collector_id = sumologic_collector.collector.id
    }
    
    resource "sumologic_http_source" "kubelet_metrics_source" {
        name         = local.kubelet_metrics_source
        collector_id = sumologic_collector.collector.id
    }
    
    resource "sumologic_http_source" "node_metrics_source" {
        name         = local.node_metrics_source
        collector_id = sumologic_collector.collector.id
    }
    
    resource "sumologic_http_source" "scheduler_metrics_source" {
        name         = local.scheduler_metrics_source
        collector_id = sumologic_collector.collector.id
    }
    
    resource "sumologic_http_source" "state_metrics_source" {
        name         = local.state_metrics_source
        collector_id = sumologic_collector.collector.id
    }
  
    resource "kubernetes_secret" "sumologic_collection_secret" {
      metadata {
        name = "sumologic"
        namespace = var.namespace_name
      }
  
      data = {
        endpoint-events                           = sumologic_http_source.default_events_source.url
        endpoint-logs                             = sumologic_http_source.default_logs_source.url
        endpoint-metrics-apiserver                = sumologic_http_source.apiserver_metrics_source.url
        endpoint-control_plane_metrics_source     = sumologic_http_source.control_plane_metrics_source.url
        endpoint-metrics-kube-controller-manager  = sumologic_http_source.controller_metrics_source.url
        endpoint-metrics                          = sumologic_http_source.default_metrics_source.url
        endpoint-metrics-kubelet                  = sumologic_http_source.kubelet_metrics_source.url
        endpoint-metrics-node-exporter            = sumologic_http_source.node_metrics_source.url
        endpoint-metrics-kube-scheduler           = sumologic_http_source.scheduler_metrics_source.url
        endpoint-metrics-kube-state               = sumologic_http_source.state_metrics_source.url
      }
  
      type = "Opaque"
    }
  setup.sh: |
    #!/bin/bash
  
    readonly DEBUG_MODE=${{DEBUG_MODE:="false"}}
    readonly DEBUG_MODE_ENABLED_FLAG="true"
  
    # Let's compare the variables ignoring the case with help of ${{VARIABLE,,}} which makes the string lowercased
    # so that we don't have to deal with True vs true vs TRUE
    if [[ ${{DEBUG_MODE,,}} == "${{DEBUG_MODE_ENABLED_FLAG}}" ]]; then
        echo "Entering the debug mode with continuous sleep. No setup will be performed."
        echo "Please exec into the setup container and run the setup.sh by hand or set the sumologic.setup.debug=false and reinstall."
  
        while true; do
            sleep 10
            echo "$(date) Sleeping in the debug mode..."
        done
    fi
  
    function fix_sumo_base_url() {{
      local BASE_URL
      BASE_URL=${{SUMOLOGIC_BASE_URL}}
  
      if [[ "${{BASE_URL}}" =~ ^\s*$ ]]; then
        BASE_URL="https://api.sumologic.com/api/"
      fi
  
      OPTIONAL_REDIRECTION="$(curl -XGET -s -o /dev/null -D - \
              -u "${{SUMOLOGIC_ACCESSID}}:${{SUMOLOGIC_ACCESSKEY}}" \
              "${{BASE_URL}}"v1/collectors \
              | grep -Fi location )"
  
      if [[ ! ${{OPTIONAL_REDIRECTION}} =~ ^\s*$ ]]; then
        BASE_URL=$( echo "${{OPTIONAL_REDIRECTION}}" | sed -E 's/.*: (https:\/\/.*(au|ca|de|eu|fed|in|jp|us2)?\.sumologic\.com\/api\/).*/\1/' )
      fi
  
      BASE_URL=${{BASE_URL%v1*}}
  
      echo "${{BASE_URL}}"
    }}
  
    SUMOLOGIC_BASE_URL=$(fix_sumo_base_url)
    export SUMOLOGIC_BASE_URL
    # Support proxy for Terraform
    export HTTP_PROXY=${{HTTP_PROXY:=""}}
    export HTTPS_PROXY=${{HTTPS_PROXY:=""}}
    export NO_PROXY=${{NO_PROXY:=""}}
  
    function get_remaining_fields() {{
        local RESPONSE
        RESPONSE="$(curl -XGET -s \
            -u "${{SUMOLOGIC_ACCESSID}}:${{SUMOLOGIC_ACCESSKEY}}" \
            "${{SUMOLOGIC_BASE_URL}}"v1/fields/quota)"
        readonly RESPONSE
  
        echo "${{RESPONSE}}"
    }}
  
    # Check if we'd have at least 10 fields remaining after additional fields
    # would be created for the collection
    function should_create_fields() {{
        local RESPONSE
        RESPONSE=$(get_remaining_fields)
        readonly RESPONSE
  
        if ! jq -e <<< "${{RESPONSE}}" ; then
            printf "Failed requesting fields API:\n%s\n" "${{RESPONSE}}"
            return 1
        fi
  
        if ! jq -e '.remaining' <<< "${{RESPONSE}}" ; then
            printf "Failed requesting fields API:\n%s\n" "${{RESPONSE}}"
            return 1
        fi
  
        local REMAINING
        REMAINING=$(jq -e '.remaining' <<< "${{RESPONSE}}")
        readonly REMAINING
        if [[ $(( REMAINING - 8 )) -ge 10 ]] ; then
            return 0
        else
            return 1
        fi
    }}
  
    cp /etc/terraform/{{locals,main,providers,resources,variables,fields}}.tf /terraform/
    cd /terraform || exit 1
  
    # Fall back to init -upgrade to prevent:
    # Error: Inconsistent dependency lock file
    terraform init -input=false -get=false || terraform init -input=false -upgrade
  
    # Sumo Logic fields
    if should_create_fields ; then
        readonly CREATE_FIELDS=1
        FIELDS_RESPONSE="$(curl -XGET -s \
            -u "${{SUMOLOGIC_ACCESSID}}:${{SUMOLOGIC_ACCESSKEY}}" \
            "${{SUMOLOGIC_BASE_URL}}"v1/fields | jq '.data[]' )"
        readonly FIELDS_RESPONSE
  
        declare -ra FIELDS=("cluster" "container" "deployment" "host" "namespace" "node" "pod" "service")
        for FIELD in "${{FIELDS[@]}}" ; do
            FIELD_ID=$( echo "${{FIELDS_RESPONSE}}" | jq -r "select(.fieldName == \"${{FIELD}}\") | .fieldId" )
            # Don't try to import non existing fields
            if [[ -z "${{FIELD_ID}}" ]]; then
                continue
            fi
  
            terraform import \
                -var="create_fields=1" \
                sumologic_field."${{FIELD}}" "${{FIELD_ID}}"
        done
    else
        readonly CREATE_FIELDS=0
        echo "Couldn't automatically create fields"
        echo "You do not have enough field capacity to create the required fields automatically."
        echo "Please refer to https://help.sumologic.com/docs/manage/fields/ to manually create the fields after you have removed unused fields to free up capacity."
    fi
  
    readonly COLLECTOR_NAME="{eks_cluster}"
  
    # Sumo Logic Collector and HTTP sources
    # Only import sources when collector exists.
    if terraform import sumologic_collector.collector "${{COLLECTOR_NAME}}"; then
    true  # prevent to render empty if; then
    terraform import sumologic_http_source.default_events_source "${{COLLECTOR_NAME}}/events"
    terraform import sumologic_http_source.default_logs_source "${{COLLECTOR_NAME}}/logs"
    terraform import sumologic_http_source.apiserver_metrics_source "${{COLLECTOR_NAME}}/apiserver-metrics"
    terraform import sumologic_http_source.control_plane_metrics_source "${{COLLECTOR_NAME}}/control-plane-metrics"
    terraform import sumologic_http_source.controller_metrics_source "${{COLLECTOR_NAME}}/kube-controller-manager-metrics"
    terraform import sumologic_http_source.default_metrics_source "${{COLLECTOR_NAME}}/(default-metrics)"
    terraform import sumologic_http_source.kubelet_metrics_source "${{COLLECTOR_NAME}}/kubelet-metrics"
    terraform import sumologic_http_source.node_metrics_source "${{COLLECTOR_NAME}}/node-exporter-metrics"
    terraform import sumologic_http_source.scheduler_metrics_source "${{COLLECTOR_NAME}}/kube-scheduler-metrics"
    terraform import sumologic_http_source.state_metrics_source "${{COLLECTOR_NAME}}/kube-state-metrics"
    fi
  
    # Kubernetes Secret
    terraform import kubernetes_secret.sumologic_collection_secret fluentd/sumologic
  
    # Apply planned changes
    TF_LOG_PROVIDER=DEBUG terraform apply \
        -auto-approve \
        -var="create_fields=${{CREATE_FIELDS}}" \
        || {{ echo "Error during applying Terraform changes"; exit 1; }}
  
    # Setup Sumo Logic monitors if enabled
    bash /etc/terraform/monitors.sh
  
    # Setup Sumo Logic dashboards if enabled
    bash /etc/terraform/dashboards.sh
  
    # Cleanup env variables
    export SUMOLOGIC_BASE_URL=
    export SUMOLOGIC_ACCESSKEY=
    export SUMOLOGIC_ACCESSID=
  
    bash /etc/terraform/custom.sh
  variables.tf: |
    variable "collector_name" {{
      type  = string
      default = "{eks_cluster}"
    }}
  
    variable "namespace_name" {{
      type  = string
      default = "fluentd"
    }}
  
    variable "create_fields" {{
      description = "If set, Terraform will attempt to create fields at Sumo Logic"
      type        = bool
      default     = true
    }}